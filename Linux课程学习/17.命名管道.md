# 1.原理
>匿名管道 : 是Linux中, 只能用于**具有亲缘关系的**进程之间,进行单向数据通信的一种方式
>进程间通信的前提是 : **多个进程之间要能看到同一份共享资源**
>匿名管道是通过**继承**的方式让子进程看到父进程创建的管道文件, 来实现资源共享的
>为什么叫匿名呢?因为没有使用路径

匿名管道最大的缺点在于, 共享区域的访问只能是两个**具有亲缘关系**的进程
如果是两个任意的进程  该如何实现通信呢? -- 使用 命名管道(FIFO)
![[Pasted image 20251208165248.png]]

**mkfifo : 创建命名管道**
![[Pasted image 20251208165346.png]]![[Pasted image 20251208165442.png]]
* 创建的文件是一个类型为 P 的特殊文件

![[Pasted image 20251208165954.png]]


**unlink : 关闭管道文件**
>unlink()  deletes  a name from the filesystem.  If that name was the last link to a file and no processes have the file open, the file is deleted and the space it was using is made available for reuse.
If  the name referred to a socket, FIFO, or device, the name for it is removed but processes which have the object open
       may continue to use it

* unlink只是从文件系统中删除一个**名字**, 如果这个名字没有链接关系(dentry中的文件名和inode的映射关系, 硬链接)且没有进程拥有这个文件的打开关系, 这个文件本身就会被放弃
* 如果关闭的是一个FIFO(命名管道), 名字被删除, 但是拥有该FIFO的进程仍然是打开的, 那么他可以继续使用


# 2.代码实现
>1. Server创建命名管道文件, 共两个进程之间进行通信
>2. Server以读的方式打开文件, Client以写的方式打开文件
>3. 进行通信操作
![[Pasted image 20251210113521.png]]
DefaultFd的作用 --> **未打开fd的标记**
1. open有可能打开失败, 导致没有文件被打开, 没有被分配fd, 直接close会有系统调用错误
2. 设置为-1,是因为fd分配时是按照下标进行分配的, 下标最小是0
![[Pasted image 20251210113458.png]]

**mkfifo进一步理解**
```c
int mkfifo(const char *pathname, mode_t mode);
```

```
       A  FIFO  special file is similar to a pipe, except that it is created in a
       different way.  Instead of being an anonymous  communications  channel,  a
       FIFO special file is entered into the filesystem by calling mkfifo().
       
              Once  you  have  created  a FIFO special file in this way, any process can open it for reading or writing, in the same way as an ordinary file.  How‐
       ever, it has to be open at both ends **simultaneously** before you can proceed
       to do any input or output operations on it.  Opening a  FIFO  for  reading
       normally  blocks until some other process opens the same FIFO for writing,
       and vice versa.  See fifo(7) for  nonblocking  handling  of  FIFO  special
       files.
```
* 和管道类似, 只是创建方式不同; 匿名管道不会进入文件系统, 而FIFO会通过mkfifo进入文件系统
* FIFO一旦被创建, 就等价于一个**普通文件**, 所有的进程都可以访问, 是文件就有自己的fd, 后续open.write操作都可以通过fd进行
* **simultaneously**, 写端和读端必须同时被打开, 缺少任意的一端, 通信的逻辑就无法被建立, 内核会阻塞对端


>无论是匿名管道还是命名管道, 通信的方式都是单向的, 都需要一个读端, 一个写端, 只不过两个进程看到共享资源的道路不同
>除此之外, 还发现, 总是需要一个进程专门负责**管理共享资源**的生命周期(创建和销毁, 一般都是读端), 另一个进程只负责写入数据即可
>下面的共享内存也是相同的方式
# 3.共享内存
>本地进程通信方案的代码 : System V(是一种标准) IPC  共享内存  信号量  消息队列

共享内存的方案和**动态库的加载**本质是一样的, 动态库被OS加载到物理内存中, 使用动态库的进程通过地址映射就可以找到动态库在物理内存上的位置(共享内存的起始虚拟地址 + 偏移量)

那么, 也就可以在物理内存上专门开辟一段共享内存, 用于进程间通信, 
关键在于, **如何让两个独立的进程找到属于他们的共享内存呢**, 想到使用类似于pid这种唯一标识符去定位

既然是一种通信的方式, 在内存中肯定有多个进程之间需要进行通信, 那么OS就需要去管理这些**共享内存** 先描述,再组织

在Linux中使用**struct Shm**描述一个共享内存, 通过双向链表管理
![[Pasted image 20251210203608.png]]
* 创建共享内存这个操作, 一定是通过**system call**实现的

>共享内存相关接口

**shmget : 创建/分配共享内存**
```c
int shmget(key_t key, size_t size, int shmflg);
```
* key : 用户形成的, 通过ftok函数创建的一个共享内存的标识符
* size : 共享内存的大小
* shmflg : 共享内存标记位

`key`
ftok  - convert a pathname and a project identifier to a System V IPC key
```c
key_t ftok(const char *pathname, int proj_id);
```
* 使用提供的文件名(pathname, 必须存在)和项目id(不能为0), 利用ftok算法, 创建出符合System V标准的IPC key
* 如果创建成功, 返回key值; 如果失败, 返回-1

`shmflg`
![[Pasted image 20251210204911.png]]
* 不存在就创建新的共享内存并返回; 存在直接返回已存在的shm(保证一定拿到)
![[Pasted image 20251210205024.png]]
* 不存在就创建一个新的shm并返回;如果存在, 直接报错(保证拿到的是全新的)

最重要的两个标记位, 一般是混着用,即 `IPC_CREAT | IPC_EXCL`, 保证一定拿到全新的共享内存

**shmget 的返回值 shmid**
`shmid : 一个有效的共享内存标识符`
![[Pasted image 20251210210529.png]]
和key的区别:
* shmid : 是用户通过系统调用, 由内核返回给用户, 用于标识共享内存, 是实际操作共享内存的句柄
* key : 是用户交给OS, 交给内核使用, 告诉内核我该怎么找到这个**共享内存**

**shmctl : 控制共享内存段**
```c
int shmctl(int shmid, int cmd, struct shmid_ds *buf);
```
* cmd : 对shmid执行什么操作  IPC_RMID 删除共享内存段
![[Pasted image 20251210215013.png]]
* 将shm**标记为被删除**, 直到最后一个进程取消挂载shm, 这个shm才会真正被删除
共享内存不随着进程的退出而释放, 是由OS创建并组织管理的, 生命周期是**随内核的**
![[Pasted image 20251214104026.png]]
* 即使进程退出, 创建的shm仍然存在, 必须手动进行删除
* ipcs -m : 查看有哪些shm
* ipcrm -m shmid : 删除shm

> shm的挂载 : OS在内存中创建了shm,没有和process链接
> 挂载 : 将shm的物理内存页和process的进程地址空间进行一一映射(mm_struct)

shmat
```c
void* shmat(int shmid, const void* shmaddress);
int shmdt(const void* shmaddress);
```

>以上都是准备工作, 接下来开始通信

```c++
// client
int main()

{
        // 1.创建shm
        Shm shm(gPathName, gProjId, User);
        // 2.获取shm地址
        char* shmaddr = shm.GetShmAddress();
        // 3.通信
        char ch = 'A';
        while(true)
        {

                shmaddr[ch -  'A'] = ch;
                ch++;
                sleep(2);
        }
}

// server
int main()

{
    // 1.创建共享内存
    Shm shm(gPathName, gProjId, Creator);
    // 2.获取shm地址
    char* shmaddr = shm.GetShmAddress();
    // 3.开始通信
    while(true)
    {

        std::cout << "shm read : " << shmaddr << std::endl;
        sleep(1);
    }
}
```

![[Pasted image 20251214110819.png]]

1. 即使client没有写入数据, server也一直在读取数据, 不同于管道(读空, server会阻塞), **少了对于数据的保护,可能会存在数据不一致的问题**
2. 把shmaddress当做一个字符数组, 可以直接进行数据的读取和写入, 没有write和read等操作(不存在系统调用)
3. shm其实是IPC最快的一种方式, 直接在shm上操作, 没有将数据拷贝到内核缓冲区的操作[[为什么shm是最快的IPC通信]]

shm的缺陷  没有对数据进行保护  + 管道实现保护
>想实现 ;我写完, 你再去进行读取; 我让你读你在进行读取

shm没有**可靠的事件传输机制**, 不知道什么时候读取数据, 如何解决呢?
1. 轮询查看 : 一直扫描shm中是否有数据刷新 -- 浪费cpu资源
2. sleep() -- 会带来数据更新的延迟
3. fifo ( 管道 ) -- 仅做提醒

当shm中有数据, write端通过向管道中写入数据, 读端阻塞失效, 开始从shm中读取数据
fifo充当**敲门通知**
![[Pasted image 20251214150504.png]]

![[Pasted image 20251214150410.png]]


# 消息队列
>原理 : 一个进程, 向另一个进程发送**有类型**数据块的方式, 实现不同进程之间的通信

![[Pasted image 20251214152030.png]]

>接口 :

```c
// 1.msgget  创建一个消息队列  key值和shm的key一样
int msgget(key_t key, int msgflg);

// 2.msgctl  对消息队列进行控制  传入标记位控制  释放/创建方式
int msgctl(int msqid, int cmd, struct msqid_ds *buf);

// 3.发送消息
int msgsnd(int msqid, const void *msgp, size_t msgsz, int msgflg);

// 4.接收消息  需要指定要接受消息的数据类型
ssize_t msgrcv(int msqid, void *msgp, size_t msgsz, long msgtyp,
                      int msgflg);
```
![[Pasted image 20251214152343.png]]
* 消息队列的生命周期也是**随内核**的

# 信号量的理解(初步)
## 五个概念
1. 共享资源 : 多个执行流能看到同一份资源
2. 临界资源 : 被保护起来的资源, 通过同步和互斥实现, 用互斥的方式保护共享资源
3. 互斥 : 任意时刻, 都只能有一个进程访问共享资源
4. 资源 : 要被程序员访问, 只能通过代码访问
5. 对共享资源的保护, 就是对**访问共享资源的代码**进行的保护

## 信号量的理解
![[Pasted image 20251214153219.png]]
* 让执行流和申请的共享资源一一对应

让不同的进程之间看到同一份计数器 -- 访问同一个的计数器 -- 计数器本身就是个**共享资源**, 如何保证计数器的安全呢?**PV操作是原子的**

申请信号量 --
访问共享内存
释放信号量 ++

信号量本身也是一个共享资源
需要让所有人都看到信号量 -- 也要是安全的 -- PV操作实现原子性, 保护共享资源   


## 信号量的操作
```c
// 获取信号量  
int semget(key_t key, int nsems, int semflg);

// 对信号量操作
int semctl(int semid, int semnum, int cmd, ...);
```

![[Pasted image 20251214153524.png]]



# Linux中System V如何统一
>System V标准的三种表达方式 : 
>1. SHM : 共享内存
>2. msg : 消息队列
>3. Semphore : 信号量
>类似点 :
>都是结构体, xxx_ds结尾; 结构体的第一个属性都是xxx_perm, 这个xxx_perm实际上就是ipc_perm的**子类**

![[Pasted image 20251214153836.png]]

![[Pasted image 20251214152536.png]]
**总结 : 实现"不同进程看到同一份共享资源的方式"**
1. 匿名管道 : 通过**fork()**, 让子进程继承父进程创建的匿名管道, 实现两个亲缘关系的进程看到同一份资源
2. 命名管道 :  通过**文件路径**, 保证两个进程打开的是同一份文件(mkfifo)
3. 共享内存 : 通过**key**保证两个进程访问的是一块共享区域(ftok算法  文件名 + proj id)



# 再次理解Linux如何统一管理System V IPC
>统一的ids管理结构 -- ipc_ids表

```c
struct ipc_ids {
    int in_use;            // 当前表中对象数量
    int max_id;            // 最大使用的 id
    struct rw_semaphore rwsem;
    struct idr ipcs_idr;   // id → 对象 的映射结构
};
```
也就是说, 每一种IPC类型, 都有自己的ipc_ids表去维护!

>统一的key ->id映射
>shmget, semget, msgget

```c
int shmget(key_t key, size_t size, int shmflg);
int msgget(key_t key, int msgflg);
int semget(key_t key, int nsems, int semflg);
```
* key (用户侧) : 由ftok + 权限组成, 内核产生, 交给用户
* id(内核测, 返回值) : ipc管理的真实句柄  用户产生,交给内核

>统一的权限模型 : struct kern_ipc_perm

```c
struct kern_ipc_perm {
    key_t   key;       // key（用户可见）
    uid_t   uid;       // 所有者
    gid_t   gid;       // 所属组
    umode_t mode;      // 权限位（0666 等）
    int     seq;       // 序列号，防止过期 id 重用
};

```

>统一的删除语义 " 延迟删除 + 引用计数"
![[Pasted image 20251214154829.png]]
* 只是被**标记为**被删除, 真正的删除是在最后一个进程取消挂载(从底层上, 就是shmid_ds的引用计数为0)
>具有统一的生命周期, 随内核
