# 1.使用第三方集成 ChatOpenAi
> Class ChatOpenAi base BaseOpenAi, 是openai模型调用的接口

`参数`
![[Pasted image 20260101151407.png]]

![[Pasted image 20260101151428.png]]
instantiate
```python
# 创建智谱 LLM 实例  
model = ChatOpenAI(  
    temperature=0,  
    model="glm-4.7",  
    api_key="9a16e9bb1599476a9cde541defeb2354.FdxzCA5dyg7UXwmG",  
    base_url="https://open.bigmodel.cn/api/paas/v4/",  
    max_tokens = 100  
)
```

* 注 : ChatOpenAi创建model,本质上还是第三方集成
# 2.init_chat_model  langchain的上层封装
`init_chat_model`
```python
init_chat_model(
    model: str | None = None,
    *,
    model_provider: str | None = None,
    configurable_fields: Literal["any"] | list[str] | tuple[str, ...] | None = None,
    config_prefix: str | None = None,
    **kwargs: Any,
) -> BaseChatModel | _ConfigurableModel
```
1. **Fixed model** – specify the model upfront and get back a ready-to-use chat model.
2. **Configurable model** – choose to specify parameters (including model name) at runtime via `config`. Makes it easy to switch between models/providers without changing your code
instantiate
```python
dsModel = init_chat_model(  
    model = "deepseek-chat",  
    model_provider = "deepseek",  
    temperature = 0.5,  
    api_key = "sk-fbfc2cfa257f4815b667579490ad984c"  
)
```
* 最大的特点是:`允许运行时指定模型参数, 包括模型名称
instantiate
```python
# (We don't need to specify configurable=True if a model isn't specified.)  
configurable_model = init_chat_model(temperature=0)  
  
resp = configurable_model.invoke(  
    "what's your name",  
    config={"configurable": {  
        "model": "deepseek-chat",  
        "model_provider": "deepseek",  
    }}  
)
```
* 设置好环境变量后需要重新启动pycharm,因为pycharm是作为一个进程启动的,会继承当前系统的环境变量, 重新设置后, 需要重新启动继承新的系统环境变量
# 3.拉取本地部署模型
```
ollama pull gpt-oss:20b // 拉取需要的模型
pip install -U langchain-ollama
```

instantiate
```python
model = ChatOllama(  
    model="deepseek-r1:1.5b",  
    validate_model_on_init=True,  
    temperature=0.8,  
    num_predict=256,  
    # other params ...  
)
```