* API接入
* 本地部署
* SDK接入

# 本地接入
>将开源的LLM部署到自己的硬件环境上, 核心概念是 : 将下载模型的文件(权重和配置文件), 使用专门的推理框架在本地服务器或GPU上加载并运行模型,然后通过类似API的方式进行交互

>Ollama：⾮常⽤⼾友好，可以⼀键拉取和运⾏模型，适合快速⼊⻔和本地开发。是一款`推理框架`
ollama下载地址 : https://ollama.ai/
ollama模型地址 : https://ollama.com/search
http://127.0.0.1:11434/

```js
ollama run deepseek-r1:1.5b
```
![[Pasted image 20251228160753.png]]

# SDK接入
>SDK(SoftWare Development Kit)软件开发工具包,就是对API的一种封装
```python
from openai import OpenAI  
  
client = OpenAI(  
    api_key="sk-fbfc2cfa257f4815b667579490ad984c",  # 直接写你的 DeepSeek API key    base_url="https://api.deepseek.com"  
)  
  
response = client.chat.completions.create(  
    model="deepseek-chat",  
    messages=[  
        {"role": "system", "content": "You are a helpful assistant"},  
        {"role": "user", "content": "Hello"},  
    ],  
    stream=False  
)  
  
print(response.choices[0].message.content)
```

# 接入方式如何选择
* 数据敏感 -- 本地部署
* 需要大规模使用 -- 本地部署可能比长时间的使用APi调用更经济
* 如果想利用自己的数据进行`微调`, 需要本地部署

# 理解LLM的局限性
1. 原生的LLM都有输入长度限制, 对于大规模文本的处理很不好
2. 本质上是一种`一问一答`的模式, 对于复杂任务的处理不是很好,需要人为控制中间状态
3. 缺乏私有知识 : 模型背后的知识库是海量且有时间限制的, 不能实现定制化回答
4. 输出格式不可控 : 虽然可以通过提⽰词要求模型输出JSON或特定格式，但它仍可能产⽣格式错误 或不合规的内容，需要我们⾃⼰编写后处理代码来校验和清洗。