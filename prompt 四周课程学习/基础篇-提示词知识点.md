**token**
自然语言--》tokenizer--》tokens
大模型中语言的基本单位， 输入输出的基本单位

**prompt**
一段文本，语句，用于指导机器学习模型生成特定类型、主题或格式的内容


**context**
> 上下文， 让模型能记得住“聊天历史”或者任务的相关信息

比如你要写一个两数之和的代码， 最开始给你写了一个暴力解法， 你接下来输入`请给我生成一个优化版本的` ,如果没有context， 模型就不知道给谁生成优化版本，需要你重新输入prompt`请生成两数之和的优化版本` , 效率就会大大降低， 正是由于context的存在，才使得模型能够记忆上下文，提高交互效率， 体验也会更好

**两个重要的超参**
大模型生成内容是一个**概率采样(sampling)** 的过程
* top-p  控制采样空间, 丰富度,按照概率的方式选择结果,先在众多候选集中选取一定阈值的候选集  
* temperature 控制多样性, 越高越随机,对于上述通过top-p挑选出的候选集,要随机选取  

以上两个参数决定了模型回答内容的**丰富性和随机性**
![[Pasted image 20250731115049.png]]

* 一般来说,越是一些比较发散的场景(创作)其`top-p和temperature` 值越高,反之越低
* 比如我现在测试的comate项目,属于`Code Generation` ,两个采纳数的值都很低
> des:产生代码依赖于坚固,稳定的模式和案例;输出是更加**确定性和聚焦性的**,有利于产生句法正确的代码

所以说,模型内部的逻辑并不是**硬编码**的,是{词 by 词}的预测,每个token都是概率采样得到的

`temperature` 
* high:在采样空间内,每个token的概率趋于一致,会产生更具**创造性**的内容,但是会牺牲一部分的**连贯性和准确性**
* low:在采样空间内,每个token的概率不一致,倾向选择较高概率的token,生成的文本会更加的连贯和准确,适用于**需要更高的准确性和事实响应的任务(ai coding)**
